{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnMc9K9_GCtO"
      },
      "source": [
        "# <font color='#fd79a8'>  Lemmatization  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-sjxwYOMZ5O"
      },
      "source": [
        "### <font color='#2d3436'>  Libraries used: NLTK's WordNet, spaCy, TextBlob, Pattern & Standford Core NLP  <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDTHva3B2w9Z",
        "outputId": "302a1bfa-ab26-4f41-8b27-6f61d3cf9e69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/shakabrah/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sNwVbJ_PaBi",
        "outputId": "d452f06f-a299-4236-c0b9-1f868cb787a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stockings => stocking\n"
          ]
        }
      ],
      "source": [
        "lem = WordNetLemmatizer()\n",
        "token = \"stockings\"\n",
        "\n",
        "result_lemma = lem.lemmatize(token)\n",
        "print(token, \"=>\", result_lemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ooZuwB_IEo9"
      },
      "source": [
        "<font color='#fd79a8'>Above, an object of the WordNetLemmatizer class was created; the object was named lem <br/>Then the word that needs to be lemmatized is passed to the lemmatize() method of the WordNetLemmatizer object - lem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82VQFl1EdYC4"
      },
      "source": [
        "<font color='#feca57'> Lemmatize an entire sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5LmYbaSdSrj",
        "outputId": "f9e86802-765b-4743-b2e2-5e531f1877f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/shakabrah/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string1 = \"The girls sang louder. The bankers banked at other banks.\" \n",
        "string2 = \"These were better shoes for her feet. The grocer was stocking the shelves at the grocery\" \n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ1z1BkA_lat",
        "outputId": "c332299f-ed4a-41b6-dec8-1524778bb3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sentence:\n",
            "['The', 'girls', 'sang', 'louder', '.', 'The', 'bankers', 'banked', 'at', 'other', 'banks', '.']\n",
            "Lemmatized sentence:\n",
            "The girl sang louder . The banker banked at other bank .\n"
          ]
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize(string1)\n",
        "print(\"Tokenized sentence:\")\n",
        "print(tokens) \n",
        "\n",
        "#Lemmatize the tokenized sentences \n",
        "lemmatized_tokens = ' '.join([lem.lemmatize(w) for w in tokens])\n",
        "print(\"Lemmatized sentence:\")\n",
        "print(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG62I1oKDWFU",
        "outputId": "30c6d032-bb9b-4ea4-c50f-b7910af1f512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sentence:\n",
            "['These', 'were', 'better', 'shoes', 'for', 'her', 'feet', '.', 'The', 'grocer', 'was', 'stocking', 'the', 'shelves', 'at', 'the', 'grocery']\n",
            "Lemmatized sentence:\n",
            "These were better shoe for her foot . The grocer wa stocking the shelf at the grocery\n"
          ]
        }
      ],
      "source": [
        "tokens2 = nltk.word_tokenize(string2)\n",
        "print(\"Tokenized sentence:\")\n",
        "print(tokens2) \n",
        "\n",
        "#Lemmatize the tokenized sentences \n",
        "lemmatized_tokens2 = ' '.join([lem.lemmatize(w) for w in tokens2])\n",
        "print(\"Lemmatized sentence:\")\n",
        "print(lemmatized_tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzGq5yHBD48d"
      },
      "source": [
        "<font color='#48dbfb'>These **be** better show for her foot. <br>\n",
        "The grocer **be stock** the shelf at the grocery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvJNOTDElqgV"
      },
      "source": [
        "<font color='#fd79a8'>word  => lemma <br/>\n",
        "sung  => sing (verb) <br/> \n",
        "are   => be (to be) (verb) <br>\n",
        "were/was => be (verb) <br/>\n",
        "<font color='#0abde3'>\n",
        "banker - noun (lemma - banker)<br>\n",
        "banked - verb (lemma - bank) <br>\n",
        "banks - noun (lemma bank) <br>\n",
        "stocking - verb (lemma - stock)<br>\n",
        "stockings - noun (lemma - stocking)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZc6Nqqz1JAE"
      },
      "source": [
        "<font color='#fd79a8'> A second argument must be added to lemmatize() - to include (or to tag) the Parts-of-Speech. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lemmatization_complete.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
