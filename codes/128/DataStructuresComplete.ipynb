{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxOkcvB9sqy6"
      },
      "source": [
        "<font color=\"#00d2d3\">Build Vocabulary - aka Preprocessing Words into Cleaned Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVQ6RanQYPdp"
      },
      "source": [
        "#<font color='#4cd137'> Bag oF Words Architecture: <br>Tokenize - Create Vocab - Count Frequency of Tokens - Create Vectorizaed Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TzCx_Vrwf46L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QJ-FmV5cgAJE"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VE-9F3jpgFZF"
      },
      "outputs": [],
      "source": [
        "text = [\n",
        "  'A rose is a flower',\n",
        "  'A fern is not a flower',\n",
        "  'the dog ate the rose',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWtfOhYsjEBX"
      },
      "source": [
        "<font color='#4cd137'> Create a pandas series object from the list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUBN1gj7gFfv",
        "outputId": "1163ac03-d712-41b6-d1bd-00d729762184"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        A rose is a flower\n",
              "1    A fern is not a flower\n",
              "2      the dog ate the rose\n",
              "dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_vocab = pd.Series(text)\n",
        "corpus_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQC-5EYpjtj_"
      },
      "source": [
        "[Keras Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)<br> tf.keras.preprocessing.text.Tokenizer \n",
        "<br> method fit_on_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NHzcf-YSgJpt"
      },
      "outputs": [],
      "source": [
        "model = Tokenizer()\n",
        "model.fit_on_texts(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5veICZSjjYxb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRng1MUuoAr7"
      },
      "source": [
        "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n",
        "\n",
        "word_counts: A dictionary of words and their counts.\n",
        "word_docs: A dictionary of words and how many documents each appeared in.\n",
        "word_index: A dictionary of words and their uniquely assigned integers.\n",
        "document_count:An integer count of the total number of documents that were used to fit the Tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwqqUi7Ygbiu",
        "outputId": "1448aab0-f316-4590-ef0f-bc961187389d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key : ['a', 'rose', 'is', 'flower', 'the', 'fern', 'not', 'dog', 'ate']\n"
          ]
        }
      ],
      "source": [
        "print(f'Key : {list(model.word_index.keys())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sqD2CfZgJvO",
        "outputId": "4ab3c186-6135-4d86-e12c-05e79afffe66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 2., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 2., 0., 1., 1., 0., 1., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 2., 0., 0., 1., 1.]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec_rep = model.texts_to_matrix(text, mode='count')\n",
        "vec_rep\n",
        "#numpy array of shape (len(texts), num_words)#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IETZjEmWhL98"
      },
      "source": [
        "The length of the vector will always be equal to vocabulary size * len document"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DataStructuresComplete.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
